---
title: "[CSC411 - Machine Learning](https://artsci.calendar.utoronto.ca/)"
collection: teaching
type: "Teaching Assistant"
permalink: /teaching/2023-summer-teaching-5
venue: "University of Toronto, Department of Mathematical and Computational Sciences"
date: 2022-09-08
location: "Toronto, Canada"
---

 # CSC 311 Fall 2022: Introduction to Machine Learning

## Overview

Machine learning (ML) is a set of techniques that allow computers to learn from data and experience, rather than requiring humans to specify the desired behaviour by hand. ML has become increasingly central both in AI as an academic field, and in industry. This course provides a broad introduction to some of the most commonly used ML algorithms. It also serves to introduce key algorithmic principles which will serve as a foundation for more advanced courses, such as CSC412/2506 (Probabilistic Learning and Reasoning) and CSC413/2516 (Neural Networks and Deep Learning).

We start with nearest neighbors, the canonical nonparametric model. We then turn to parametric models: linear regression, logistic regression, softmax regression, and neural networks. We then move on to unsupervised learning, focusing in particular on probabilistic models, but also principal components analysis and K-means. Finally, we cover the basics of reinforcement learning.

## Announcements

- Homework 4 is posted.
- Final project is posted.
- Homework 3 is posted.
- Homework 2 is posted.
- The TAs will host additional office hours to help review linear algebra.
- Homework 1 is posted.
- Instructor office hours are posted. Please see Quercus for instructions on how to attend.
- The course information handout (syllabus) is available.

## Where and When

Each section of this course corresponds to one lecture and one tutorial time. Class will be held synchronously online every week, including a combination of lecture and tutorial exercises. Students are encouraged to attend both the lecture and tutorial each week. There will be two mandatory tests held during the scheduled class time.

| Sections | Lecture Time | Tutorial Time |
|----------|--------------|---------------|
| LEC0101, LEC0102, LEC2001 | Monday 11-1 | Monday 3-4 |
| LEC0201, LEC0202, LEC2001 | Thursday 4-6 | Thursday 7-8 |

Online delivery. Lectures will be delivered synchronously via Zoom and recorded for asynchronous viewing by enrolled students. Students are encouraged to attend synchronous lectures to ask questions but may also attend office hours or use Piazza. All information about attending virtual lectures, tutorials, and office hours will be sent to enrolled students through Quercus.

Course videos and materials belong to your instructor, the University, and/or other source depending on the specific facts of each situation, and are protected by copyright. In this course, you are permitted to download session videos and materials for your own academic use, but you should not copy, share, or use them for any other purpose without the explicit permission of the instructor. For questions about recording and use of videos in which you appear, please contact your instructor.

## Teaching Staff

### Instructors

- Juhan Bae
- Roger Grosse
- Chris Maddison
- Silviu Pitis

| Office Hours | LEC0102 | LEC0201 | LEC0101, LEC2001 | LEC0202 |
|--------------|---------|---------|-----------------|---------|
| Thursday 2-4 | Monday 1-3 | Monday 6-8 | Friday 10-12 |

Email Instructors Only: csc311-2020-09@cs.toronto.edu

### Teaching Assistants

We will use Piazza for the course forum. If your question is about the course material and doesn't give away any hints for the homework, please post to Piazza so that.

# Schedule

This is a tentative schedule, which will likely change as the course goes on.

Suggested readings are optional; they are resources we recommend to help you understand the course material. All of the textbooks listed below are freely available online.

- Bishop = Pattern Recognition and Machine Learning, by Chris Bishop.
- ESL = The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman.
- MacKay = Information Theory, Inference, and Learning Algorithms, by David MacKay.
- Barber = Bayesian Reasoning and Machine Learning, by David Barber.
- Sutton and Barto = Reinforcement Learning: An Introduction, by Sutton and Barto.

## 1. Dates: 9/10, 9/14
   - Topic: Lecture: Introduction, Nearest Neighbours
   - Tutorial: Probability
   - Lecture Materials: [Slides]
   - Tutorial Materials: [Slides]
   - Suggested Readings:
     - ESL: 1, 2.1-2.3, 2.5
     - Domingos, 2012. A few useful things to know about machine learning.
     - Breiman, 2001. Statistical Modeling: The Two Cultures

## 2. Dates: 9/17, 9/21
   - Topic: Lecture: Linear Methods for Regression, Optimization
   - Tutorial: Linear Algebra Review
   - Lecture Materials: [Slides]
   - Tutorial Materials: [Slides]
   - Suggested Readings:
     - Bishop: 3.1
     - ESL: 3.1 - 3.2
     - Course notes: Linear Regression, Calculus

## 3. Dates: 9/24, 9/28
   - Topic: Lecture: Logistic Regression, Multiclass Classification, Optimization
   - Tutorial: Optimization
   - Lecture Materials: [Slides]
   - Tutorial Materials:
     - [Slides]
     - Tutorial: [Colab] [ipynb]
     - Worksheet: [Colab] [ipynb]
   - Suggested Readings:
     - Bishop: 4.1, 4.3
     - ESL: 4.1-4.2, 4.4, 11
     - Course notes: Linear Classifiers, Training a Classifier

## 4. Dates: 10/1, 10/5
   - Topic: Lecture: Neural Networks
   - Tutorial: PyTorch
   - Lecture Materials: [Slides]
   - Tutorial Materials:
     - Tutorial: [Colab] [ipynb]
     - Solution: [Colab] [ipynb]
   - Suggested Readings:
     - Bishop: 5.1-5.3
     - Course notes: Multilayer Perceptrons, Backpropagation

## 5. Dates: 10/08, 10/12
   - Topic: Lecture: Decision Trees, Bias-Variance Decomposition
   - Tutorial: TBA
   - Note: Lecture will be held as usual on Thanksgiving. It will be recorded as usual, so you are welcome to watch the recording instead.
   - Lecture Materials: [Slides]
   - Tutorial Materials:
     - Worksheet: [Worksheet]
     - Solution: [Slides]
   - Suggested Readings:
     - Bishop: 3.2
     - ESL: 2.9, 9.2
     - Course notes: Generalization

## 6. Dates: 10/15, 10/19
   - Topic: Lecture: Bagging, Boosting
   - Tutorial: Midterm Review
   - Lecture Materials: [Slides]
   - Tutorial Materials: [Slides]
   - Suggested Readings:
     - ESL: 8.7, 10.1-10.5

## 7. Dates: 10/22, 10/26
   - Topic: Lecture: Probabilistic Models
   - Tutorial: None (in-class test)
   - Lecture Materials: [Slides]
   - Suggested Readings:
     - ESL: 2.6.3, 6.6.3, 4.3.0
     - MacKay: 21, 23, 24
     - Course notes: Probabilistic Models

## 8. Dates: 10/29, 11/2
   - Topic: Lecture: Probabilistic Models cont'd; Principal Component Analysis
   - Tutorial: Eigenvectors, PCA
   - Lecture Materials: [Slides]
   - Tutorial Materials: [Notes]
   - Suggested Readings:
     - Bishop: 12.1

## 9. Dates: 11/5, 11/16
   - Topic: Lecture: PCA cont'd; Matrix Completion; Autoencoders
   - Tutorial: Final Project
   - Lecture Materials: [Slides]
   - Tutorial Materials: [Slides] [Colab]
   - Suggested Readings:
     - ESL: 14.5.1

## 10. Dates: 11/19, 11/23
    - Topic: Lecture: k-Means, EM Algorithm
    - Tutorial: TBA
    - Lecture Materials: [Slides]
    - Tutorial Materials: [Slides]
    - Suggested Readings:
      - MacKay: 20
      - Bishop: 9
      - Barber: 20.1-20.3
      - Course notes: Mixture Modeling

## 11. Dates: 11/27, 11/30
    - Topic: Lecture: Reinforcement learning
    - Tutorial: Test 2 Review
    - Lecture Materials: [Slides]
    - Tutorial Materials: [Slides]
    - Suggested Readings:
      - Sutton and Barto: 3, 4.1, 4.4, 6.1-6.5

## 12. Dates: 12/3, 12/7
    - Topic: Lecture: AlphaGo and Game Playing
    - Tutorial: None (in-class test)
    - Lecture Materials: [Slides]

## Final Project
25% of your total mark is allocated to a final project, which will require you to apply several algorithms to a challenge problem and to write a short report analyzing the results. The deadline for the final project is December 15th (final evaluation period). You can find the full project requirements [here](link-to-project-requirements) and starter code [here](link-to-starter-code).

## Paper Readings
5% of your total mark is allocated to reading a set of classic machine learning papers. We hope these papers are both interesting and understandable given what you learn in this course. The 5 points are allocated on an honor system; at the end of the term, you'll check a box to indicate that you've done the readings. You don't need to hand anything in, and the readings will not be tested on the exam.

- P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. CVPR 2001. [pdf]
- A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. NIPS 2012. [pdf]
- R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. NIPS 2007. [pdf]
- B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by V1? Vision Research, 1997. [pdf]
- V. Mnih et al. Human-level control through deep reinforcement learning. Nature, 2015. [article]

# Computing Resources

For the homework assignments, we will use Python 3 and libraries such as NumPy, SciPy, and scikit-learn. You have two options:

1. **Option 1: Install on Your Own Machine**
   - If you don't already have Python 3, install it.
   - We recommend using some version of Anaconda (Miniconda, a lightweight Conda, is a good choice). Alternatively, you can install Python directly if you know how.
   - Optionally, create a virtual environment for this class and activate it. If you have a Conda distribution, run the following commands:
     ```
     conda create --name csc311
     source activate csc311
     ```
   - Use pip to install the required packages:
     ```
     pip install scipy numpy autograd matplotlib jupyter sklearn
     ```

2. **Option 2: Teaching Labs Machines**
   - All the required packages are already installed on the Teaching Labs machines.
